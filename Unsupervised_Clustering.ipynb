{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "import string\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nuforc_reports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing and Vectorizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing text with summary\n",
    "df['text']= df['text'].fillna(value = df['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows where there's no summary or text\n",
    "reports = df.dropna(axis=0, how='all', subset=['text','summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing text with summary\n",
    "reports['text'].fillna(value = reports['summary'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports['text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation (text):\n",
    "  \n",
    "  new_text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "  return new_text\n",
    "\n",
    "reports['clean_text'] = reports['text'].astype(str).apply(lambda x:remove_punctuation(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize (text):\n",
    "  tokens = re.split('\\W+', text)\n",
    "  return tokens [1:]\n",
    "\n",
    "reports['tokenized_text'] = reports['clean_text'].apply(lambda x: tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(tokenized_text):\n",
    "  text = [word for word in tokenized_text if word not in stopword]\n",
    "  return text\n",
    "\n",
    "reports['nostop_text']= reports['tokenized_text'].apply(lambda x:remove_stopwords(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new dataframe with vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports['date_time'] = pd.to_datetime(reports['date_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the year, month, day, hour, and minute from our datetime data.\n",
    "\n",
    "reports['year'] = reports['date_time'].map(lambda x: x.year)\n",
    "reports['month'] = reports['date_time'].map(lambda x: x.month)\n",
    "reports['day'] = reports['date_time'].map(lambda x: x.day)\n",
    "reports['hour'] = reports['date_time'].map(lambda x: x.hour)\n",
    "reports['minute'] = reports['date_time'].map(lambda x: x.minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parsing duration for parsable entries\n",
    "def extract_duration(text):\n",
    "    if(type(text) != 'str'):\n",
    "        text = str(text)\n",
    "    if(text == \"\"):\n",
    "        return -1\n",
    "    elif(not any(char.isdigit() for char in text)):\n",
    "        return -1\n",
    "    else:\n",
    "        # first numeral value\n",
    "        start_idx = None\n",
    "        end_idx = None\n",
    "        units_idx = None\n",
    "        for i in range(len(text)):\n",
    "            if(text[i].isdigit()):\n",
    "                start_idx = i\n",
    "#                 print(\"start_idx=\"+str(start_idx))\n",
    "                break\n",
    "#         print(\"end of loop start_idx:\"+str(start_idx))\n",
    "        for i in range(start_idx,len(text)):\n",
    "            if(not text[i].isdigit()):\n",
    "                end_idx = i\n",
    "#                 print(\"text[\"+str(i)+\"]=\"+str(text[i]))\n",
    "#                 print(\"end_idx=\"+str(end_idx))\n",
    "                break\n",
    "        if(end_idx == None):\n",
    "            return -1\n",
    "        duration_str = text[start_idx:end_idx]\n",
    "        print(\"duration_str=\"+str(duration_str))\n",
    "        duration = int(duration_str)\n",
    "        for i in range(end_idx,end_idx+min(5,len(text)-end_idx)):\n",
    "            if(text[i] == \"m\" or text[i] == \"M\" or text[i] == \"s\" or text[i] == \"S\" or text[i] == \"h\" or text[i] == \"H\"):\n",
    "                units_idx = i\n",
    "                break\n",
    "        if(units_idx == None):\n",
    "            return -1\n",
    "        if(text[units_idx] == \"h\" or text[units_idx] == \"H\"):\n",
    "            return duration*3600\n",
    "        if(text[units_idx] == \"m\" or text[units_idx] == \"M\"):\n",
    "            return duration*60\n",
    "        if(text[units_idx] == \"s\" or text[units_idx] == \"S\"):\n",
    "            return duration\n",
    "\n",
    "reports['duration_parsed'] = reports['duration'].apply(lambda x:extract_duration(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate location data into regions\n",
    "reports['citystate'] = reports['city'] + \"!\" + reports['state']\n",
    "\n",
    "def region_separate(citystate):\n",
    "    pacific = ['WA','OR','CA']\n",
    "    rockies = ['NV','ID','MT','WY','UT','CO']\n",
    "    southwest = ['AZ','NM','TX','OK']\n",
    "    midwest = ['ND','SD','NE','KS','MN','IA','MO','WI','IL','IN','MI','OH']\n",
    "    southeast = ['AR','LA','TN','MS','KY','AL','FL','GA','SC','NC','VA','WV','MD','DE']\n",
    "    northeast = ['PA','NY','NJ','CT','RI','MA','VT','NH','ME']\n",
    "    noncontiguous = ['AK','HI']\n",
    "    north_canada = ['YT','NU','NT']\n",
    "    british_columbia = ['BC']\n",
    "    prarie_provinces = ['AB','SK','MB']\n",
    "    ontario = ['ON']\n",
    "    quebec = ['QC']\n",
    "    atlantic_provinces = ['NL','PE','NB','NS']\n",
    "    \n",
    "    try:\n",
    "        parsed = citystate.split(\"!\")\n",
    "        if(parsed[1] in pacific):\n",
    "            return \"pacific\"\n",
    "        elif(parsed[1] in rockies):\n",
    "            return \"rockies\"\n",
    "        elif(parsed[1] in southwest):\n",
    "            return \"southwest\"\n",
    "        elif(parsed[1] in midwest):\n",
    "            return \"midwest\"\n",
    "        elif(parsed[1] in southeast):\n",
    "            return \"southeast\"\n",
    "        elif(parsed[1] in northeast):\n",
    "            return \"northeast\"\n",
    "        elif(parsed[1] in noncontiguous):\n",
    "            return \"noncontiguous\"\n",
    "        elif(parsed[1] in north_canada):\n",
    "            return \"north_canada\"\n",
    "        elif(parsed[1] in british_columbia):\n",
    "            return \"british_columbia\"\n",
    "        elif(parsed[1] in prarie_provinces):\n",
    "            return \"prarie_provinces\"\n",
    "        elif(parsed[1] in ontario):\n",
    "            return \"ontario\"\n",
    "        elif(parsed[1] in quebec):\n",
    "            return \"quebec\"\n",
    "        elif(parsed[1] in atlantic_provinces):\n",
    "            return \"atlantic_provinces\"\n",
    "        else:\n",
    "            if(\"UK/England\" in parsed[0]):\n",
    "                return \"UK\"\n",
    "            else:\n",
    "                return \"other\"\n",
    "    except:\n",
    "        return \"other\"\n",
    "        \n",
    "reports['region'] = reports['citystate'].apply(lambda x: region_separate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping similar shapes\n",
    "reports_shpgroup = reports.replace({'shape' : {'disk': 'circle','unknown':'other','teardrop':'oval',\n",
    "        'egg': 'oval','fireball': 'sphere', 'delta':'triangle', 'pyramid':'triangle', 'cigar':'cylinder',\n",
    "       'round': 'sphere', 'changed':'changing', 'flare':'light'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode our categorical variables.\n",
    "\n",
    "cat_cols = ['city', 'state', 'shape', 'region']\n",
    "\n",
    "for col in cat_cols:\n",
    "    temp = pd.get_dummies(reports_shpgroup[col], prefix=col)\n",
    "    reports_shpgroup = pd.concat([reports_shpgroup, temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep for tokenization/vectorization\n",
    "texts=[\" \".join(text) for text in reports_shpgroup['nostop_text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text vectorization at the word level \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    " \n",
    "#word level tf-idf\n",
    "Tfidf_vect = TfidfVectorizer(max_features=6000)\n",
    "\n",
    "Tfidf_vect.fit(texts)\n",
    "tfidf_vals = Tfidf_vect.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix vectors to dataframe and join original dataframe\n",
    "vectors_df = pd.DataFrame.sparse.from_spmatrix(tfidf_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_wvectors = pd.concat([reports_shpgroup, vectors_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_shpgroup.columns[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop deprecated or unnecessary columns\n",
    "reports_final = reports_wvectors.drop(columns = ['summary', 'city', 'state', 'date_time', 'shape', 'duration', 'stats',\n",
    "                                                   'report_link', 'text', 'posted', 'region',\n",
    "                                                   'clean_text', 'tokenized_text', 'nostop_text', 'citystate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Shape from Text: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_df = reports[pd.notnull(reports['shape'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shape_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_df['shape'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping similar shapes\n",
    "new_shape_df = shape_df.replace({'shape' : {'disk': 'circle','unknown':'other','teardrop':'oval',\n",
    "        'egg': 'oval','fireball': 'sphere', 'delta':'triangle', 'pyramid':'triangle', 'cigar':'cylinder',\n",
    "       'round': 'sphere', 'changed':'changing', 'flare':'light'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_shape_df['shape'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[\" \".join(text) for text in new_shape_df['nostop_text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "y= Encoder.fit_transform(new_shape_df['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(texts, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "#TEXT REPRESENTATION\n",
    "#word level tf-idf\n",
    "Tfidf_vect = TfidfVectorizer(max_features=6000)\n",
    "\n",
    "Tfidf_vect.fit(texts)\n",
    "x_train_tfidf = Tfidf_vect.transform(x_train)\n",
    "x_test_tfidf = Tfidf_vect.transform(x_test)\n",
    "\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(texts)\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(x_train)\n",
    "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(x_test)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(texts)\n",
    "X_train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(x_train) \n",
    "X_test_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(x_test) \n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=4)\n",
    "X_counts = count_vect.fit(texts)\n",
    "x_train_count =  count_vect.transform(x_train)\n",
    "x_test_count =  count_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_sparse_df = pd.DataFrame.sparse.from_spmatrix(x_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Naive Bayes on Count Vectors\n",
    "def NaiveBayes(X_train, Y_train, X_test, Y_test, vectorizer):\n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(X_train,Y_train)\n",
    "    predictions_NB = Naive.predict(X_test)\n",
    "    probs = Naive.predict_proba(X_test)\n",
    "    probability = np.amax(probs, 1)\n",
    "    print(vectorizer,\"Accuracy Score -> \",accuracy_score(predictions_NB, Y_test)*100)\n",
    "    print(metrics.classification_report(Y_test, predictions_NB))\n",
    "    summary = pd.DataFrame({'%':probability,'Prediction':predictions_NB})\n",
    "    print(summary)\n",
    "\n",
    "\n",
    "NaiveBayes(x_train_count, y_train, x_test_count, y_test, 'Count')\n",
    "NaiveBayes(x_train_tfidf, y_train, x_test_tfidf, y_test, 'Word')\n",
    "NaiveBayes(X_train_tfidf_ngram, Y_train, X_test_tfidf_ngram, Y_test, 'Ngram')\n",
    "NaiveBayes(X_train_tfidf_ngram_chars, Y_train, X_test_tfidf_ngram_chars, Y_test, 'Ngram Char')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Clustering for Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_binary = reports_final[reports_final.columns[-6000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_binary = vectors_binary.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reports_embedding = umap.UMAP(n_neighbors = 5\n",
    "                                      #,min_dist = ???\n",
    "                                      #,n_components = ???,\n",
    "                                      ,metric = 'hamming'\n",
    "                                      ).fit_transform(vectors_binary[vectors_binary.columns[:-10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set(style='white', rc={'figure.figsize':(25,25)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(reports_embedding[:,0], reports_embedding[:,1], c = vectors_binary['clust_grp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering for vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_binary = reports_final[reports_final.columns[-6000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_binary = vectors_binary.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make k-means clusterer\n",
    "from sklearn.cluster import KMeans\n",
    "clusterer = KMeans(4, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit clusterer\n",
    "clusterer.fit(vectors_binary[-6000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values\n",
    "vectors_binary['clust_grp'] = clusterer.predict(vectors_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_wclusters = pd.concat([reports, vectors_binary['clust_grp']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_wclusters_reduced = reports_wclusters[reports_wclusters['city_latitude'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_wclusters_reduced = reports_wclusters_reduced[reports_wclusters_reduced['city_longitude'] < -40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's recreate this scatterplot with plotly\n",
    "import plotly.express as px\n",
    "fig = px.scatter(reports_wclusters_reduced, x='city_longitude', y='city_latitude', color=\"clust_grp\"\n",
    "                 #,size= 'duration'\n",
    "                 #,hover_data=['Name']\n",
    "                )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Text Vector Clusters by Latitude/Longitude\",\n",
    "    xaxis_title=\"Latitude\",\n",
    "    yaxis_title=\"Longitude\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "    \n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_clusters_novectors = pd.concat([reports, vectors_binary['clust_grp']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_clusters_novectors.to_csv(r'reports_clusters_novectors.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
